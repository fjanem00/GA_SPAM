{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import re  \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import load_files  \n",
    "import pickle  \n",
    "import os.path\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text,stop_words_list):\n",
    "    \n",
    "    stop = list(stopwords.words('english'))\n",
    "    with open(stop_words_list, 'r', encoding='utf-8')as line:\n",
    "            stop = line.readlines() + stop\n",
    "    text = text.replace(r'\\b(' + r'|'.join(stop) + r')\\b\\s*', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_lem(text):\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    text = porter.stem(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_removing(text):\n",
    "    \n",
    "    text = text.lower()  \n",
    "    text = re.sub('&','and',text)\n",
    "    text = re.sub(r'\\W', ' ',text)\n",
    "    cleanhtml = re.compile('<.*?>')\n",
    "    text = re.sub(cleanhtml,' ', text)\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ',text)\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+',' ',text)\n",
    "    text = re.sub(r'(^\\-)|(\\s*)(\\-|\\^|\\+|\\*|\\–)(\\s)|(\\s)(\\-|\\^|\\+|\\*|\\–)(\\s*)',' ',text)\n",
    "    text = re.sub(r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\",'',text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_removing(text):\n",
    "    \n",
    "    text = text.replace('@','a').replace('4','A').replace('3','E').replace('¡','i').replace('!','i').replace('0','o')\n",
    "    text = text.replace('8','g').replace('6','G').replace('9','q')\n",
    "    text = text.replace('1','l').replace('|','l').replace('5','s').replace('2','s').replace('+','t').replace('7','T')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(file,cat,text, csv_file):\n",
    "    \n",
    "    with open(csv_file,'a') as writer:\n",
    "            h = str(file)+','+str(cat)+','+str(text)\n",
    "            writer.write(h)\n",
    "            writer.write('\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_preprocessing(path, stop_words_list):\n",
    "    \n",
    "    data_frame = pd.read_csv(path)\n",
    "    csv_file = \"./enronText_prepro.csv\"\n",
    "    text = ' '\n",
    "    with open(csv_file,'a') as writer:\n",
    "        h = \"Name,Category,Text\"\n",
    "        writer.write(h)\n",
    "        writer.write('\\n')  \n",
    "\n",
    "    address = data_frame['Name_File']\n",
    "    for file in address:\n",
    "        cat = data_frame[data_frame['Name_File']==file]['Category'].iloc[0]\n",
    "        if cat == 1:\n",
    "            path_spam = \"./spam/\"+file\n",
    "            if os.path.isfile(path_spam):     \n",
    "                with open(path_spam, 'r',encoding='utf-8',errors='ignore') as txt_spam:\n",
    "                    text = str(txt_spam.read()).replace('\\n','').replace(',','')  \n",
    "        else:\n",
    "            path_ham = \"./ham/\"+file\n",
    "            if os.path.isfile(path_ham): \n",
    "                with open(path_ham, 'r',errors='ignore') as txt:\n",
    "                    text = str(txt.read()).replace('\\n', '').replace(',','')\n",
    "        \n",
    "        \n",
    "        text = spam_removing(text)\n",
    "        text = general_removing(text)    \n",
    "        text = stem_lem(text)\n",
    "        text = remove_stopwords(text,stop_words_list)              \n",
    "        save_csv(file,cat,text, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_preprocessing(\"dataset_10K.csv\",\"stop_words_spam.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
